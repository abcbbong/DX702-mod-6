In all cases, written answers (apart from code) should not be longer than about three paragraphs. Graders may not read all of your submission if it is longer than that.

Homework reflection 1

1. In Coding Quiz 1, you are asked to find the distance of the farthest match in a set.  Is this farthest match distance too far to be a meaningful match?  How can you decide this?

In Coding Quiz 1, the farthest match distance (Q4) is approximately 0.2102 for the Z variable in homework_1.2.csv, where Z is uniformly distributed between ~0 and 1 (range ≈0.9837, mean ≈0.473, std ≈0.290). This distance represents about 21% of the total range and 0.72 times the standard deviation, suggesting it is moderately large and could introduce bias in causal estimates by pairing units that are substantively dissimilar, potentially violating the common support assumption in matching.

To decide if it's too far, evaluate balance metrics like the standardized mean difference (SMD) post-matching; an SMD >0.1-0.2 indicates poor balance (here, post-matching SMD ≈0.178, borderline acceptable). Additionally, conduct sensitivity analysis by varying the matching radius (e.g., restrict to 0.1) and re-estimating the effect (Q5: 0.5434); if the effect changes substantially (>10-20%), the farthest match likely adds noise. Visual diagnostics like love plots or QQ plots for covariate distributions pre/post-matching can further confirm if this match degrades overall quality, ensuring the ATT estimate remains robust.

2. In Coding Quiz 1, there are two approaches to matching: 
(A) Picking the best match X = 0 corresponding to each X = 1 using Z values.
(B) Using radius_neighbors to pick all matches X = 0 within a distance of 0.2 of each X = 1.

Invent your own type of matching similar to 1 and 2 (or look one up on the internet), which has a different way to pick the matches in X = 0.  Clearly explain the approach you invented or found.

I propose "optimal caliper matching with replacement," an extension of optimal pair matching (inspired by Hansen's optmatch algorithm), which minimizes the total sum of Z-distances across all pairs while enforcing a caliper (maximum allowable distance, e.g., 0.15 std of Z) to avoid poor matches. Unlike (A)'s greedy 1-NN (which may leave suboptimal global assignments) and (B)'s radius-based multiple matches (which allows duplicates but no optimization), this method uses bipartite graph matching (e.g., Hungarian algorithm) on a cost matrix of pairwise Euclidean distances, filtered by the caliper. It ensures 1:1 pairing per treated unit but allows replacement for controls, discarding treated units without viable matches to prioritize quality.

This balances bias and variance better in small datasets like this (n=100), reducing the Q5/Q7 effect discrepancy (0.5434 vs. 0.5844) by optimizing globally. Implementation involves computing a distance matrix, applying the caliper (set to 0.2 initially, tuned via cross-validation for balance), and solving for minimal cost assignments. Potential issues include computational cost (O(n^3)) and sensitivity to caliper width, which can be addressed by iterating over values and checking SMD.

Homework reflection 2

1. Invent an example situation that would use fixed effects.

Consider a longitudinal study of employee productivity in a tech firm with three teams developing AI models, observed over 12 months. Productivity (e.g., lines of code committed per day) varies due to time-invariant team-specific factors like leadership style, team cohesion, and tool access, which confound a simple regression of productivity on training hours (the treatment). A fixed-effects model isolates the within-team variation by assigning each team its own intercept, estimating productivity ~ training_hours + C(team), assuming a common slope for training_hours across teams.

This de-means the data at the team level, removing unobserved heterogeneity and identifying the effect from month-to-month changes within teams, under the assumption of no time-varying confounders (e.g., no differential policy changes). If significant, the training_hours coefficient represents the average treatment effect; diagnostics like Hausman tests can confirm fixed effects over random effects, and clustering standard errors at the team level accounts for serial correlation.

2. Write a Python program that performs a bootstrap simulation to find the variance in the mean of the Pareto distribution when different samples are taken.  Explain what you had to do for this.  As you make the full sample size bigger (for the same distribution), what happens to the variance of the mean of the samples?  Does it stay about the same, get smaller, or get bigger?

To simulate, I generated Pareto samples (shape α=2.5 >2 for finite variance; scale xm=1) using NumPy's pareto, then bootstrapped the sample mean's variance by resampling with replacement B=3000 times per original sample of size n, repeating R=20 times for stability. The code uses a seeded RNG for reproducibility, computes Var(bootstrap means), and plots trends across n values (50 to 1000). For heavy tails (α=1.5 ≤2), variance is infinite, so estimates may not converge.

As n increases, the variance of the sample mean decreases (≈σ²/n, where σ² is finite for α>2), following the central limit theorem; for α≤2, it remains unstable or grows due to extreme outliers. This highlights bootstrap's utility for heavy-tailed distributions where analytic variances fail.


import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

SEED = 42  # Fixed seed for reproducibility

def pareto_sample(alpha=2.5, xm=1.0, n=2000, rng=None):
    """Draw n samples from Pareto(alpha, xm) with support [xm, ∞)."""
    assert rng is not None, "Provide a numpy Generator (rng) for reproducibility."
    return (rng.pareto(alpha, size=n) + 1.0) * xm

def bootstrap_var_of_mean(x, B=3000, rng=None):
    """Bootstrap estimate of Var(mean) from data x."""
    assert rng is not None, "Provide a numpy Generator (rng) for reproducibility."
    n = len(x)
    means = np.empty(B)
    for b in range(B):
        idx = rng.integers(0, n, size=n)  # Resample with replacement
        means[b] = x[idx].mean()
    return means.var(ddof=1)  # Variance across bootstrap means

def run_experiment(alpha=2.5, xm=1.0, sizes=(50,100,250,500,1000),
                   B=3000, R=20, rng=None):
    """
    For each n, draw an original Pareto sample, compute bootstrap Var(mean),
    repeat R times, and return list of (n, average variance, sd across repeats).
    """
    assert rng is not None, "Provide a numpy Generator (rng) for reproducibility."
    results = []
    for n in sizes:
        vals = np.array([bootstrap_var_of_mean(pareto_sample(alpha=alpha, xm=xm, n=n, rng=rng), B=B, rng=rng)
                         for _ in range(R)])
        results.append((n, vals.mean(), vals.std(ddof=1)))
    return results

def plot_results(results, title, outfile):
    """Plot n vs bootstrap variance of mean. Saves to outfile."""
    n_vals, var_means = [r[0] for r in results], [r[1] for r in results]
    plt.figure()
    plt.plot(n_vals, var_means, marker="o")
    plt.xlabel("Sample size n")
    plt.ylabel("Bootstrap Var(mean)")
    plt.title(title)
    plt.grid(True)
    plt.savefig(outfile, dpi=150, bbox_inches="tight")
    plt.show()

if __name__ == "__main__":
    rng = np.random.default_rng(SEED)  # Single RNG for entire script
    # Finite-variance case
    res = run_experiment(alpha=2.5, xm=1.0, rng=rng)
    print("alpha=2.5 (finite variance):")
    for n, vbar, sd in res:
        print(f"n={n:4d}  boot Var(mean) ≈ {vbar:.6f}  (SD {sd:.6f})")
    # Heavy-tail case
    res_heavy = run_experiment(alpha=1.5, xm=1.0, rng=rng)
    print("\nalpha=1.5 (very heavy tail):")
    for n, vbar, sd in res_heavy:
        print(f"n={n:4d}  boot Var(mean) ≈ {vbar:.6f}  (SD {sd:.6f})")
    # Charts
    outdir = Path("/mnt/data")
    outdir.mkdir(parents=True, exist_ok=True)
    plot_results(res, "Pareto (alpha = 2.5): Bootstrap variance of the sample mean vs n (seed=42)",
                 outdir / "pareto_alpha25_varmean_seed42.png")
    plot_results(res_heavy, "Pareto (alpha = 1.5): Bootstrap variance of the sample mean vs n (seed=42)",
                 outdir / "pareto_alpha15_varmean_seed42.png")


Homework reflection 3

1. In the event study in Coding Quiz 3, how would we go about testing for a change in the second derivative as well?

To test for a second-derivative discontinuity (change in curvature) at the event (time=50), extend the linear model by adding quadratic terms: value ~ time_centered + event + time_event_interaction + time_centered_sq + time_centered_sq_event, where time_centered_sq = time_centered² and time_centered_sq_event = time_centered² * event. The coefficient on time_centered_sq_event estimates the change in the second derivative; a significant t-statistic (or p<0.05) indicates a shift in trend acceleration post-event.

This quadratic specification captures non-linear trends, improving model fit if higher-order discontinuities exist (e.g., accelerating growth). Validate with F-tests for joint significance of interactions and residuals checks for remaining non-linearity; if needed, higher polynomials or splines could extend this, but overfitting risks increase.

2. Create your own scenario that illustrates differences-in-differences. Describe the story behind the data and show whether there is a nonzero treatment effect.

In an automotive tech firm, a new AI-based diagnostic tool is rolled out to the Berlin factory (treatment group) to reduce assembly line downtime, while the Munich factory (control) continues with manual diagnostics. Data tracks average daily downtime hours before (6 months) and after (6 months) rollout; Berlin pre: 4.2 hours, post: 2.1 hours; Munich pre: 4.0 hours, post: 3.9 hours. Parallel trends hold pre-treatment, with no major confounders like weather differences.

Using DiD regression: downtime ~ treatment + post + treatment_post (clustered SEs at factory level), the treatment_post coefficient is -2.0 (SE=0.4, p<0.001), indicating a significant nonzero treatment effect of 2-hour reduction attributable to the tool. This assumes no spillovers; robustness checks like placebo tests on pre-data confirm validity.

Homework reflection 4

1. The Coding Quiz gives two options for instrumental variables.  For the second item (dividing the range of W into multiple ranges), explain how you did it, show your code, and discuss any issues you encountered.

For the stratified IV approach, I binned the confounder W into 10 equal-width intervals using pd.cut to condition on narrow W ranges, reducing bias from W's confounding. Within each bin, I computed the Wald estimator (ΔY/ΔX by Z=1 vs. Z=0), then averaged across bins for the overall effect, handling NaNs from empty subgroups.

Issues include small-sample volatility in sparse bins (e.g., division by zero if ΔX=0), addressed by skipping NaNs but potentially biasing toward populated ranges; bin count sensitivity (too many: variance inflation; too few: residual confounding). Tune bins via cross-validation for balance, or use kernel weighting for smoother conditioning.


import pandas as pd
import numpy as np

# Load the dataset
df1 = pd.read_csv('homework_4.1.csv', index_col=0)

# Wald estimator function
def wald_estimator(df):
    df_z1 = df[df['Z'] == 1]
    df_z0 = df[df['Z'] == 0]
    if len(df_z1) == 0 or len(df_z0) == 0:
        return np.nan
    diff_y = df_z1['Y'].mean() - df_z0['Y'].mean()
    diff_x = df_z1['X'].mean() - df_z0['X'].mean()
    return np.nan if diff_x == 0 else diff_y / diff_x

# Bin W into 10 equal-width bins
df1['w_bins'] = pd.cut(df1['W'], bins=10)

# Compute Wald per bin and average (ignoring NaNs)
binned_effects = df1.groupby('w_bins', observed=True).apply(wald_estimator)
final_effect = binned_effects.mean(skipna=True)

print("Causal effects per bin of W:")
print(binned_effects)
print(f"\nFinal Average Causal Effect: {final_effect:.4f}")


2. Plot the college outcome (Y) vs. the test score (X) in a small range of test scores around 80. On the plot, compare it with the Y probability predicted by logistic regression. The ground truth Y value is 0 or 1; don't just plot 0 or 1 - that will make it unreadable.  Find some way to make it look better than that.

To enhance readability, I filtered data to X in [75,85], binned X into 20 intervals, and plotted binned means of Y (empirical admission probability) as a scatter. Overlaid is the logistic regression's predicted probability curve, highlighting the RDD discontinuity at X=80 that logistic misses due to its smoothness assumption.

This visualization underscores RDD's strength in capturing sharp causal effects, with the jump evident in data but smoothed in logistic—ideal for validating fuzzy RDD assumptions.


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression

# Load dataset
df_a = pd.read_csv('homework_4.2.a.csv', index_col=0)

# Filter around cutoff=80
cutoff, plot_range = 80, 5
df_subset = df_a[(df_a['X'] >= cutoff - plot_range) & (df_a['X'] <= cutoff + plot_range)].copy()

# Bin X for readable scatter (means represent empirical probs)
bins = np.linspace(df_subset['X'].min(), df_subset['X'].max(), 20)
df_subset['x_bins'] = pd.cut(df_subset['X'], bins=bins)
binned_data = df_subset.groupby('x_bins', observed=True).agg({'X': 'mean', 'Y': 'mean'}).dropna()

# Fit logistic
X, y = df_subset[['X']], df_subset['Y']
model = LogisticRegression()
model.fit(X, y)

# Predict smooth curve
x_fit = np.linspace(df_subset['X'].min(), df_subset['X'].max(), 300).reshape(-1, 1)
y_fit_prob = model.predict_proba(x_fit)[:, 1]

# Plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x='X', y='Y', data=binned_data, color='blue', s=100, label='Empirical Prob (Binned Avg)')
plt.plot(x_fit, y_fit_prob, color='red', linestyle='--', label='Logistic Prediction')
plt.axvline(x=cutoff, color='black', linestyle='-.', label=f'Cutoff = {cutoff}')
plt.title('Admission Probability vs. Test Score (RDD Around Cutoff)')
plt.xlabel('Test Score (X)')
plt.ylabel('Admission Probability (Y)')
plt.legend()
plt.grid(True)
plt.savefig('rdd_logistic_comparison.png', dpi=150)
